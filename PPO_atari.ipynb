{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Hyperparameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_timesteps = 10000000 # total timesteps of the experiment\n",
    "learning_rate = 2.5e-4 # the learning rate of the optimizer\n",
    "num_envs = 8 # the number of parallel environments\n",
    "num_steps = 128 # the number of steps to run in each environment per policy rollout\n",
    "gamma = 0.99 # the discount factor gamma\n",
    "gae_lambda = 0.95 # the lambda for the general advantage estimation\n",
    "num_minibatches = 4 # the number of mini batches\n",
    "update_epochs = 4 # the K epochs to update the policy\n",
    "clip_coef = 0.1 # the surrogate clipping coefficient\n",
    "ent_coef = 0.01 # coefficient of the entropy\n",
    "vf_coef = 0.5 # coefficient of the value function\n",
    "max_grad_norm = 0.5 # the maximum norm for the gradient clipping\n",
    "seed = 1 # seed for reproducible benchmarks\n",
    "exp_name = 'PPO' # unique experiment name\n",
    "env_id= \"KungFuMasterNoFrameskip-v4\" # id of the environment\n",
    "capture_video = True # whether to save video of agent gameplay\n",
    "\n",
    "batch_size = num_envs * num_steps # size of the batch after one rollout\n",
    "minibatch_size = batch_size // num_minibatches # size of the mini batch\n",
    "num_updates = total_timesteps // batch_size # the number of learning cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Disabling gpu on tensorflow (it is used internally by some flax modules)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-10 21:03:15.690172: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-10 21:03:16.446739: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-04-10 21:03:17.598713: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-04-10 21:03:17.617083: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-04-10 21:03:17.617283: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.config.experimental.set_visible_devices([], 'GPU')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Make environment"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import string\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "def make_env(env_id: string, idx: int, capture_video: bool, run_name: string):\n",
    "    def thunk():\n",
    "        if capture_video:\n",
    "            env = gym.make(env_id, render_mode='rgb_array')\n",
    "            if idx == 0:\n",
    "                env = gym.wrappers.RecordVideo(env, f\"videos/{env_id}\")\n",
    "        else:\n",
    "            env = gym.make(env_id)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        env = gym.wrappers.AtariPreprocessing(env, grayscale_newaxis=True, scale_obs=True)\n",
    "        return env\n",
    "\n",
    "    return thunk"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n",
      "/home/adrian/.venv/lib/python3.10/site-packages/gymnasium/wrappers/record_video.py:87: UserWarning: \u001B[33mWARN: Overwriting existing videos at /home/adrian/Projekty/PPO/videos/KungFuMasterNoFrameskip-v4 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001B[0m\n",
      "  logger.warn(\n",
      "/home/adrian/.venv/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:364: UserWarning: \u001B[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001B[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "envs = gym.vector.SyncVectorEnv(\n",
    "    [make_env(env_id, i, capture_video, exp_name) for i in range(num_envs)]\n",
    ") # AsyncVectorEnv is faster, but we cannot extract single environment from it\n",
    "assert isinstance(envs.single_action_space, gym.spaces.Discrete), \"only discrete action space is supported\"\n",
    "obs, _ = envs.reset()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create Agent model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import flax.linen as nn\n",
    "\n",
    "# Helper function to quickly declare linear layer with weight and bias initializers\n",
    "def linear_layer_init(features, std=np.sqrt(2), bias_const=0.0):\n",
    "    layer = nn.Dense(features=features, kernel_init=nn.initializers.orthogonal(std), bias_init=nn.initializers.constant(bias_const))\n",
    "    return layer\n",
    "\n",
    "# Helper function to quickly declare convolution layer with weight and bias initializers\n",
    "def convolution_layer_init(features, kernel_size, strides, std=np.sqrt(2), bias_const=0.0):\n",
    "    layer = nn.Conv(features=features, kernel_size=(kernel_size, kernel_size), strides=(strides, strides), padding='VALID', kernel_init=nn.initializers.orthogonal(std), bias_init=nn.initializers.constant(bias_const))\n",
    "    return layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from jax import Array\n",
    "import jax.numpy as jnp\n",
    "\n",
    "class Network(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x: Array):\n",
    "        x = nn.Sequential([\n",
    "            convolution_layer_init(32, 8, 4),\n",
    "            nn.relu,\n",
    "            convolution_layer_init(64, 4, 2),\n",
    "            nn.relu,\n",
    "            convolution_layer_init(64, 3, 1),\n",
    "            nn.relu,\n",
    "        ])(x)\n",
    "        x = jnp.reshape(x, (x.shape[0], -1))\n",
    "        return nn.Sequential([\n",
    "            linear_layer_init(512),\n",
    "            nn.relu\n",
    "        ])(x)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    action_n: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x: Array):\n",
    "        return linear_layer_init(self.action_n, std=0.01)(x)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x: Array):\n",
    "        return linear_layer_init(1, std=1)(x)\n",
    "\n",
    "network = Network()\n",
    "actor = Actor(action_n=envs.single_action_space.n) # For jit we need to declare prod outside of class\n",
    "critic = Critic()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create AgentState"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import jax.random as random\n",
    "\n",
    "# Setting seed of the environment for reproduction\n",
    "key = random.PRNGKey(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "key, network_key, actor_key, critic_key, action_key, permutation_key = random.split(key, num=6)\n",
    "\n",
    "# Initializing agent parameters\n",
    "network_params = network.init(network_key, obs)\n",
    "\n",
    "logits = network.apply(network_params, obs)\n",
    "\n",
    "actor_params = actor.init(actor_key, logits)\n",
    "critic_params = critic.init(critic_key, logits)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import optax\n",
    "\n",
    "# Anneal learning rate over time\n",
    "def linear_schedule(count):\n",
    "    frac = 1.0 - (count // (num_minibatches * update_epochs)) / num_updates\n",
    "    return learning_rate * frac\n",
    "\n",
    "tx = optax.chain(\n",
    "    optax.clip_by_global_norm(max_grad_norm),\n",
    "    optax.inject_hyperparams(optax.adamw)(\n",
    "        learning_rate=linear_schedule,\n",
    "        eps=1e-5\n",
    "    )\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from flax.core import FrozenDict\n",
    "from flax.struct import dataclass\n",
    "\n",
    "@dataclass\n",
    "class AgentParams:\n",
    "    actor_params: FrozenDict\n",
    "    critic_params: FrozenDict\n",
    "    network_params: FrozenDict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from jax import jit\n",
    "from typing import Callable\n",
    "from flax.training.train_state import TrainState\n",
    "from flax import struct\n",
    "\n",
    "# Probably jitting isn't needed as this functions should be jitted already\n",
    "actor.apply = jit(actor.apply)\n",
    "critic.apply = jit(critic.apply)\n",
    "network.apply = jit(network.apply)\n",
    "\n",
    "class AgentState(TrainState):\n",
    "    # Setting default values for agent functions to make TrainState work in jitted function\n",
    "    actor_fn: Callable = struct.field(pytree_node=False)\n",
    "    critic_fn: Callable = struct.field(pytree_node=False)\n",
    "    network_fn: Callable = struct.field(pytree_node=False)\n",
    "\n",
    "agent_state = AgentState.create(\n",
    "    params=AgentParams(\n",
    "        network_params=network_params,\n",
    "        actor_params=actor_params,\n",
    "        critic_params=critic_params\n",
    "    ),\n",
    "    tx=tx,\n",
    "    # As we have separated actor and critic we don't use apply_fn\n",
    "    apply_fn=None,\n",
    "    actor_fn=actor.apply,\n",
    "    critic_fn=critic.apply,\n",
    "    network_fn=network.apply\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Only run this if you want to continue training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "tx = optax.chain(\n",
    "    optax.clip_by_global_norm(max_grad_norm),\n",
    "    optax.inject_hyperparams(optax.adamw)(\n",
    "        learning_rate=linear_schedule,\n",
    "        eps=1e-5\n",
    "    )\n",
    ")\n",
    "\n",
    "agent_state = AgentState.create(\n",
    "    params=AgentParams(\n",
    "        actor_params=agent_state.params.actor_params,\n",
    "        critic_params=agent_state.params.critic_params,\n",
    "        network_params=agent_state.params.network_params\n",
    "    ),\n",
    "    tx=tx,\n",
    "    apply_fn=None,\n",
    "    actor_fn=actor.apply,\n",
    "    critic_fn=critic.apply,\n",
    "    network_fn=network.apply\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create storage"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Storage:\n",
    "    obs: jnp.array\n",
    "    actions: jnp.array\n",
    "    logprobs: jnp.array\n",
    "    dones: jnp.array\n",
    "    values: jnp.array\n",
    "    advantages: jnp.array\n",
    "    returns: jnp.array\n",
    "    rewards: jnp.array"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sample action"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from numpy import ndarray\n",
    "import tensorflow_probability.substrates.jax.distributions as tfp\n",
    "\n",
    "@jit\n",
    "def get_action_and_value(agent_state: AgentState, next_obs: ndarray, next_done: ndarray, storage: Storage, step: int, key: random.PRNGKeyArray):\n",
    "    hidden = agent_state.network_fn(agent_state.params.network_params, next_obs)\n",
    "    action_logits = agent_state.actor_fn(agent_state.params.actor_params, hidden)\n",
    "    value = agent_state.critic_fn(agent_state.params.critic_params, hidden)\n",
    "\n",
    "    # Sample discrete actions from Normal distribution\n",
    "    probs = tfp.Categorical(action_logits)\n",
    "    key, subkey = random.split(key)\n",
    "    action = probs.sample(seed=subkey)\n",
    "    logprob = probs.log_prob(action)\n",
    "    storage = storage.replace(\n",
    "        obs=storage.obs.at[step].set(next_obs),\n",
    "        dones=storage.dones.at[step].set(next_done),\n",
    "        actions=storage.actions.at[step].set(action),\n",
    "        logprobs=storage.logprobs.at[step].set(logprob),\n",
    "        values=storage.values.at[step].set(value.squeeze()),\n",
    "    )\n",
    "    return storage, action, key"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "@jit\n",
    "def get_action_and_value2(agent_state: AgentState, params: AgentParams, obs: ndarray, action: ndarray):\n",
    "    hidden = agent_state.network_fn(params.network_params, obs)\n",
    "    action_logits = agent_state.actor_fn(params.actor_params, hidden)\n",
    "    value = agent_state.critic_fn(params.critic_params, hidden)\n",
    "\n",
    "    probs = tfp.Categorical(action_logits)\n",
    "    return probs.log_prob(action), probs.entropy(), value.squeeze()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Rollout"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "from flax.metrics.tensorboard import SummaryWriter\n",
    "from jax import device_get\n",
    "\n",
    "def rollout(\n",
    "        agent_state: AgentState,\n",
    "        next_obs: ndarray,\n",
    "        next_done: ndarray,\n",
    "        storage: Storage,\n",
    "        key: random.PRNGKeyArray,\n",
    "        global_step: int,\n",
    "        writer: SummaryWriter,\n",
    "):\n",
    "    for step in range(0, num_steps):\n",
    "        global_step += 1 * num_envs\n",
    "        storage, action, key = get_action_and_value(agent_state, next_obs, next_done, storage, step, key)\n",
    "        next_obs, reward, terminated, truncated, infos = envs.step(device_get(action))\n",
    "        next_done = terminated | truncated\n",
    "        storage = storage.replace(rewards=storage.rewards.at[step].set(reward))\n",
    "\n",
    "        # Only print when at least 1 env is done\n",
    "        if \"final_info\" not in infos:\n",
    "            continue\n",
    "\n",
    "        for info in infos[\"final_info\"]:\n",
    "            # Skip the envs that are not done\n",
    "            if info is None:\n",
    "                continue\n",
    "            writer.scalar(\"charts/episodic_return\", info[\"episode\"][\"r\"], global_step)\n",
    "            writer.scalar(\"charts/episodic_length\", info[\"episode\"][\"l\"], global_step)\n",
    "    return next_obs, next_done, storage, key, global_step"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Compute gae"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "@jit\n",
    "def compute_gae(\n",
    "        agent_state: AgentState,\n",
    "        next_obs: ndarray,\n",
    "        next_done: ndarray,\n",
    "        storage: Storage\n",
    "):\n",
    "    # Reset advantages values\n",
    "    storage = storage.replace(advantages=storage.advantages.at[:].set(0.0))\n",
    "    hidden = agent_state.network_fn(agent_state.params.network_params, next_obs)\n",
    "    next_value = agent_state.critic_fn(agent_state.params.critic_params, hidden).squeeze()\n",
    "    # Compute advantage using generalized advantage estimate\n",
    "    lastgaelam = 0\n",
    "    for t in reversed(range(num_steps)):\n",
    "        if t == num_steps - 1:\n",
    "            nextnonterminal = 1.0 - next_done\n",
    "            nextvalues = next_value\n",
    "        else:\n",
    "            nextnonterminal = 1.0 - storage.dones[t + 1]\n",
    "            nextvalues = storage.values[t + 1]\n",
    "        delta = storage.rewards[t] + gamma * nextvalues * nextnonterminal - storage.values[t]\n",
    "        lastgaelam = delta + gamma * gae_lambda * nextnonterminal * lastgaelam\n",
    "        storage = storage.replace(advantages=storage.advantages.at[t].set(lastgaelam))\n",
    "    # Save returns as advantages + values\n",
    "    storage = storage.replace(returns=storage.advantages + storage.values)\n",
    "    return storage"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PPO loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "from jax.lax import stop_gradient\n",
    "\n",
    "@jit\n",
    "def ppo_loss(\n",
    "        agent_state: AgentState,\n",
    "        params: AgentParams,\n",
    "        obs: ndarray,\n",
    "        act: ndarray,\n",
    "        logp: ndarray,\n",
    "        adv: ndarray,\n",
    "        ret: ndarray,\n",
    "        val: ndarray,\n",
    "):\n",
    "    newlogprob, entropy, newvalue = get_action_and_value2(agent_state, params, obs, act)\n",
    "    logratio = newlogprob - logp\n",
    "    ratio = jnp.exp(logratio)\n",
    "\n",
    "    # Calculate how much policy is changing\n",
    "    approx_kl = ((ratio - 1) - logratio).mean()\n",
    "\n",
    "    # Advantage normalization\n",
    "    adv = (adv - adv.mean()) / (adv.std() + 1e-8)\n",
    "\n",
    "    # Policy loss\n",
    "    pg_loss1 = -adv * ratio\n",
    "    pg_loss2 = -adv * jnp.clip(ratio, 1 - clip_coef, 1 + clip_coef)\n",
    "    pg_loss = jnp.maximum(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "    # Value loss\n",
    "    v_loss_unclipped = (newvalue - ret) ** 2\n",
    "    v_clipped = val + jnp.clip(\n",
    "        newvalue - val,\n",
    "        -clip_coef,\n",
    "        clip_coef,\n",
    "    )\n",
    "    v_loss_clipped = (v_clipped - ret) ** 2\n",
    "    v_loss_max = jnp.maximum(v_loss_unclipped, v_loss_clipped)\n",
    "    v_loss = 0.5 * v_loss_max.mean()\n",
    "\n",
    "    # Entropy loss\n",
    "    entropy_loss = entropy.mean()\n",
    "\n",
    "    # main loss as sum of each part loss\n",
    "    loss = pg_loss - ent_coef * entropy_loss + v_loss * vf_coef\n",
    "    return loss, (pg_loss, v_loss, entropy_loss, stop_gradient(approx_kl))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Update PPO"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "from jax import value_and_grad\n",
    "\n",
    "\n",
    "def update_ppo(\n",
    "        agent_state: AgentState,\n",
    "        storage: Storage,\n",
    "        key: random.PRNGKeyArray\n",
    "):\n",
    "    # Flatten collected experiences\n",
    "    b_obs = storage.obs.reshape((-1,) + envs.single_observation_space.shape)\n",
    "    b_logprobs = storage.logprobs.reshape(-1)\n",
    "    b_actions = storage.actions.reshape((-1,) + envs.single_action_space.shape)\n",
    "    b_advantages = storage.advantages.reshape(-1)\n",
    "    b_returns = storage.returns.reshape(-1)\n",
    "    b_values = storage.values.reshape(-1)\n",
    "\n",
    "    # Create function that will return gradient of the specified function\n",
    "    ppo_loss_grad_fn = jit(value_and_grad(ppo_loss, argnums=1, has_aux=True))\n",
    "\n",
    "    for epoch in range(update_epochs):\n",
    "        key, subkey = random.split(key)\n",
    "        b_inds = random.permutation(subkey, batch_size, independent=True)\n",
    "        for start in range(0, batch_size, minibatch_size):\n",
    "            end = start + minibatch_size\n",
    "            mb_inds = b_inds[start:end]\n",
    "            (loss, (pg_loss, v_loss, entropy_loss, approx_kl)), grads = ppo_loss_grad_fn(\n",
    "                agent_state,\n",
    "                agent_state.params,\n",
    "                b_obs[mb_inds],\n",
    "                b_actions[mb_inds],\n",
    "                b_logprobs[mb_inds],\n",
    "                b_advantages[mb_inds],\n",
    "                b_returns[mb_inds],\n",
    "                b_values[mb_inds],\n",
    "            )\n",
    "            # Update an agent\n",
    "            agent_state = agent_state.apply_gradients(grads=grads)\n",
    "\n",
    "    # Calculate how good an approximation of the return is the value function\n",
    "    y_pred, y_true = b_values, b_returns\n",
    "    var_y = jnp.var(y_true)\n",
    "    explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "    return agent_state, loss, pg_loss, v_loss, entropy_loss, approx_kl, explained_var, key"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train agent"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33majaskowiec\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.14.2 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.14.1"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/home/adrian/Projekty/PPO/wandb/run-20230410_153003-7brddjua</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/ajaskowiec/KungFuMasterNoFrameskip-v4/runs/7brddjua' target=\"_blank\">PPO_1_Mon_Apr_10_15:30:02_2023</a></strong> to <a href='https://wandb.ai/ajaskowiec/KungFuMasterNoFrameskip-v4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/ajaskowiec/KungFuMasterNoFrameskip-v4' target=\"_blank\">https://wandb.ai/ajaskowiec/KungFuMasterNoFrameskip-v4</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/ajaskowiec/KungFuMasterNoFrameskip-v4/runs/7brddjua' target=\"_blank\">https://wandb.ai/ajaskowiec/KungFuMasterNoFrameskip-v4/runs/7brddjua</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-10 15:30:04.929799: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/9765 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1e7cb4e460a648a99b6954733888224a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /home/adrian/Projekty/PPO/videos/KungFuMasterNoFrameskip-v4/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /home/adrian/Projekty/PPO/videos/KungFuMasterNoFrameskip-v4/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "t:   0%|          | 0/3338 [00:00<?, ?it/s, now=None]\u001B[A\n",
      "t:  10%|█         | 341/3338 [00:00<00:00, 3407.58it/s, now=None]\u001B[A\n",
      "t:  20%|██        | 682/3338 [00:00<00:00, 3361.76it/s, now=None]\u001B[A\n",
      "t:  31%|███       | 1019/3338 [00:00<00:00, 3316.33it/s, now=None]\u001B[A\n",
      "t:  41%|████      | 1374/3338 [00:00<00:00, 3406.35it/s, now=None]\u001B[A\n",
      "t:  51%|█████▏    | 1715/3338 [00:00<00:00, 3314.69it/s, now=None]\u001B[A\n",
      "t:  61%|██████▏   | 2047/3338 [00:00<00:00, 3293.31it/s, now=None]\u001B[A\n",
      "t:  71%|███████▏  | 2386/3338 [00:00<00:00, 3320.73it/s, now=None]\u001B[A\n",
      "t:  82%|████████▏ | 2723/3338 [00:00<00:00, 3333.55it/s, now=None]\u001B[A\n",
      "t:  92%|█████████▏| 3057/3338 [00:00<00:00, 3258.51it/s, now=None]\u001B[A\n",
      "                                                                  \u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/adrian/Projekty/PPO/videos/KungFuMasterNoFrameskip-v4/rl-video-episode-0.mp4\n",
      "Moviepy - Building video /home/adrian/Projekty/PPO/videos/KungFuMasterNoFrameskip-v4/rl-video-episode-1.mp4.\n",
      "Moviepy - Writing video /home/adrian/Projekty/PPO/videos/KungFuMasterNoFrameskip-v4/rl-video-episode-1.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "t:   0%|          | 0/2530 [00:00<?, ?it/s, now=None]\u001B[A\n",
      "t:  17%|█▋        | 433/2530 [00:00<00:00, 4327.48it/s, now=None]\u001B[A\n",
      "t:  34%|███▍      | 866/2530 [00:00<00:00, 4248.43it/s, now=None]\u001B[A\n",
      "t:  51%|█████     | 1291/2530 [00:00<00:00, 4164.52it/s, now=None]\u001B[A\n",
      "t:  68%|██████▊   | 1711/2530 [00:00<00:00, 4177.36it/s, now=None]\u001B[A\n",
      "t:  84%|████████▍ | 2129/2530 [00:00<00:00, 4075.22it/s, now=None]\u001B[A\n",
      "                                                                  \u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/adrian/Projekty/PPO/videos/KungFuMasterNoFrameskip-v4/rl-video-episode-1.mp4\n",
      "Moviepy - Building video /home/adrian/Projekty/PPO/videos/KungFuMasterNoFrameskip-v4/rl-video-episode-8.mp4.\n",
      "Moviepy - Writing video /home/adrian/Projekty/PPO/videos/KungFuMasterNoFrameskip-v4/rl-video-episode-8.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "t:   0%|          | 0/4383 [00:00<?, ?it/s, now=None]\u001B[A\n",
      "t:   7%|▋         | 318/4383 [00:00<00:01, 3179.25it/s, now=None]\u001B[A\n",
      "t:  15%|█▍        | 643/4383 [00:00<00:01, 3219.14it/s, now=None]\u001B[A\n",
      "t:  22%|██▏       | 965/4383 [00:00<00:01, 3187.47it/s, now=None]\u001B[A\n",
      "t:  29%|██▉       | 1284/4383 [00:00<00:00, 3100.03it/s, now=None]\u001B[A\n",
      "t:  36%|███▋      | 1595/4383 [00:00<00:00, 3102.65it/s, now=None]\u001B[A\n",
      "t:  43%|████▎     | 1906/4383 [00:00<00:00, 3072.68it/s, now=None]\u001B[A\n",
      "t:  51%|█████     | 2214/4383 [00:00<00:00, 3043.29it/s, now=None]\u001B[A\n",
      "t:  57%|█████▋    | 2519/4383 [00:00<00:00, 2994.61it/s, now=None]\u001B[A\n",
      "t:  64%|██████▍   | 2819/4383 [00:00<00:00, 2972.53it/s, now=None]\u001B[A\n",
      "t:  71%|███████   | 3117/4383 [00:01<00:00, 2899.57it/s, now=None]\u001B[A\n",
      "t:  78%|███████▊  | 3408/4383 [00:01<00:00, 2821.44it/s, now=None]\u001B[A\n",
      "t:  84%|████████▍ | 3691/4383 [00:01<00:00, 2808.98it/s, now=None]\u001B[A\n",
      "t:  91%|█████████ | 3973/4383 [00:01<00:00, 2792.59it/s, now=None]\u001B[A\n",
      "t:  97%|█████████▋| 4253/4383 [00:01<00:00, 2768.90it/s, now=None]\u001B[A\n",
      "                                                                  \u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/adrian/Projekty/PPO/videos/KungFuMasterNoFrameskip-v4/rl-video-episode-8.mp4\n",
      "Moviepy - Building video /home/adrian/Projekty/PPO/videos/KungFuMasterNoFrameskip-v4/rl-video-episode-27.mp4.\n",
      "Moviepy - Writing video /home/adrian/Projekty/PPO/videos/KungFuMasterNoFrameskip-v4/rl-video-episode-27.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "t:   0%|          | 0/5519 [00:00<?, ?it/s, now=None]\u001B[A\n",
      "t:   5%|▍         | 259/5519 [00:00<00:02, 2582.29it/s, now=None]\u001B[A\n",
      "t:   9%|▉         | 518/5519 [00:00<00:01, 2569.42it/s, now=None]\u001B[A\n",
      "t:  14%|█▍        | 775/5519 [00:00<00:01, 2462.34it/s, now=None]\u001B[A\n",
      "t:  19%|█▊        | 1032/5519 [00:00<00:01, 2502.16it/s, now=None]\u001B[A\n",
      "t:  23%|██▎       | 1288/5519 [00:00<00:01, 2522.62it/s, now=None]\u001B[A\n",
      "t:  28%|██▊       | 1541/5519 [00:00<00:01, 2474.00it/s, now=None]\u001B[A\n",
      "t:  33%|███▎      | 1794/5519 [00:00<00:01, 2489.76it/s, now=None]\u001B[A\n",
      "t:  37%|███▋      | 2045/5519 [00:00<00:01, 2493.24it/s, now=None]\u001B[A\n",
      "t:  42%|████▏     | 2296/5519 [00:00<00:01, 2497.01it/s, now=None]\u001B[A\n",
      "t:  46%|████▌     | 2546/5519 [00:01<00:01, 2486.24it/s, now=None]\u001B[A\n",
      "t:  51%|█████     | 2798/5519 [00:01<00:01, 2495.60it/s, now=None]\u001B[A\n",
      "t:  55%|█████▌    | 3048/5519 [00:01<00:01, 2449.38it/s, now=None]\u001B[A\n",
      "t:  60%|█████▉    | 3294/5519 [00:01<00:00, 2434.10it/s, now=None]\u001B[A\n",
      "t:  64%|██████▍   | 3539/5519 [00:01<00:00, 2437.46it/s, now=None]\u001B[A\n",
      "t:  69%|██████▊   | 3783/5519 [00:01<00:00, 2359.83it/s, now=None]\u001B[A\n",
      "t:  73%|███████▎  | 4020/5519 [00:01<00:00, 2289.48it/s, now=None]\u001B[A\n",
      "t:  77%|███████▋  | 4257/5519 [00:01<00:00, 2311.50it/s, now=None]\u001B[A\n",
      "t:  81%|████████▏ | 4489/5519 [00:01<00:00, 2306.67it/s, now=None]\u001B[A\n",
      "t:  86%|████████▌ | 4721/5519 [00:01<00:00, 2290.24it/s, now=None]\u001B[A\n",
      "t:  90%|████████▉ | 4951/5519 [00:02<00:00, 2278.00it/s, now=None]\u001B[A\n",
      "t:  94%|█████████▍| 5179/5519 [00:02<00:00, 2266.78it/s, now=None]\u001B[A\n",
      "t:  98%|█████████▊| 5406/5519 [00:02<00:00, 2250.43it/s, now=None]\u001B[A\n",
      "                                                                  \u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/adrian/Projekty/PPO/videos/KungFuMasterNoFrameskip-v4/rl-video-episode-27.mp4\n",
      "Moviepy - Building video /home/adrian/Projekty/PPO/videos/KungFuMasterNoFrameskip-v4/rl-video-episode-64.mp4.\n",
      "Moviepy - Writing video /home/adrian/Projekty/PPO/videos/KungFuMasterNoFrameskip-v4/rl-video-episode-64.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "t:   0%|          | 0/5487 [00:00<?, ?it/s, now=None]\u001B[A\n",
      "t:   4%|▍         | 244/5487 [00:00<00:02, 2436.71it/s, now=None]\u001B[A\n",
      "t:   9%|▉         | 516/5487 [00:00<00:01, 2600.64it/s, now=None]\u001B[A\n",
      "t:  14%|█▍        | 793/5487 [00:00<00:01, 2673.70it/s, now=None]\u001B[A\n",
      "t:  19%|█▉        | 1061/5487 [00:00<00:01, 2654.58it/s, now=None]\u001B[A\n",
      "t:  24%|██▍       | 1327/5487 [00:00<00:01, 2554.25it/s, now=None]\u001B[A\n",
      "t:  29%|██▉       | 1584/5487 [00:00<00:01, 2529.43it/s, now=None]\u001B[A\n",
      "t:  33%|███▎      | 1838/5487 [00:00<00:01, 2504.50it/s, now=None]\u001B[A\n",
      "t:  38%|███▊      | 2089/5487 [00:00<00:01, 2480.09it/s, now=None]\u001B[A\n",
      "t:  43%|████▎     | 2338/5487 [00:00<00:01, 2445.66it/s, now=None]\u001B[A\n",
      "t:  47%|████▋     | 2583/5487 [00:01<00:01, 2376.01it/s, now=None]\u001B[A\n",
      "t:  52%|█████▏    | 2833/5487 [00:01<00:01, 2412.38it/s, now=None]\u001B[A\n",
      "t:  56%|█████▌    | 3075/5487 [00:01<00:00, 2413.51it/s, now=None]\u001B[A\n",
      "t:  60%|██████    | 3317/5487 [00:01<00:00, 2410.55it/s, now=None]\u001B[A\n",
      "t:  65%|██████▍   | 3559/5487 [00:01<00:00, 2398.18it/s, now=None]\u001B[A\n",
      "t:  69%|██████▉   | 3799/5487 [00:01<00:00, 2384.77it/s, now=None]\u001B[A\n",
      "t:  74%|███████▎  | 4038/5487 [00:01<00:00, 2363.54it/s, now=None]\u001B[A\n",
      "t:  78%|███████▊  | 4275/5487 [00:01<00:00, 2359.05it/s, now=None]\u001B[A\n",
      "t:  82%|████████▏ | 4511/5487 [00:01<00:00, 2345.25it/s, now=None]\u001B[A\n",
      "t:  86%|████████▋ | 4746/5487 [00:01<00:00, 2331.34it/s, now=None]\u001B[A\n",
      "t:  91%|█████████ | 4984/5487 [00:02<00:00, 2342.80it/s, now=None]\u001B[A\n",
      "t:  95%|█████████▌| 5219/5487 [00:02<00:00, 2328.63it/s, now=None]\u001B[A\n",
      "t:  99%|█████████▉| 5452/5487 [00:02<00:00, 2301.29it/s, now=None]\u001B[A\n",
      "                                                                  \u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/adrian/Projekty/PPO/videos/KungFuMasterNoFrameskip-v4/rl-video-episode-64.mp4\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import wandb\n",
    "from tqdm.notebook import tqdm\n",
    "from termcolor import colored\n",
    "import signal\n",
    "\n",
    "# Make kernel interrupt be handled as normal python error\n",
    "signal.signal(signal.SIGINT, signal.default_int_handler)\n",
    "\n",
    "run_name = f\"{exp_name}_{seed}_{time.asctime(time.localtime(time.time())).replace('  ', ' ').replace(' ', '_')}\"\n",
    "wandb.init(\n",
    "    project=env_id,\n",
    "    sync_tensorboard=True,\n",
    "    name=run_name,\n",
    "    save_code=True,\n",
    "    monitor_gym=capture_video,\n",
    "    config={\n",
    "        'total_timesteps': total_timesteps,\n",
    "        'learning_rate': learning_rate,\n",
    "        'num_envs': num_envs,\n",
    "        'num_steps': num_steps,\n",
    "        'gamma': gamma,\n",
    "        'gae_lambda': gae_lambda,\n",
    "        'num_minibatches': num_minibatches,\n",
    "        'update_epochs': update_epochs,\n",
    "        'clip_coef': clip_coef,\n",
    "        'ent_coef': ent_coef,\n",
    "        'vf_coef': vf_coef,\n",
    "        'max_grad_norm': max_grad_norm,\n",
    "        'seed': seed,\n",
    "        'batch_size': batch_size,\n",
    "        'minibatch_size': minibatch_size,\n",
    "        'num_updates': num_updates,\n",
    "    }\n",
    ")\n",
    "writer = SummaryWriter(f'runs/{env_id}/{run_name}')\n",
    "\n",
    "# Initialize the storage\n",
    "storage = Storage(\n",
    "    obs=jnp.zeros((num_steps, num_envs) + envs.single_observation_space.shape),\n",
    "    actions=jnp.zeros((num_steps, num_envs) + envs.single_action_space.shape),\n",
    "    logprobs=jnp.zeros((num_steps, num_envs)),\n",
    "    dones=jnp.zeros((num_steps, num_envs)),\n",
    "    values=jnp.zeros((num_steps, num_envs)),\n",
    "    advantages=jnp.zeros((num_steps, num_envs)),\n",
    "    returns=jnp.zeros((num_steps, num_envs)),\n",
    "    rewards=jnp.zeros((num_steps, num_envs)),\n",
    ")\n",
    "global_step = 0\n",
    "start_time = time.time()\n",
    "next_obs, _ = envs.reset(seed=seed)\n",
    "next_done = jnp.zeros(num_envs)\n",
    "\n",
    "try:\n",
    "    for update in tqdm(range(1, num_updates + 1)):\n",
    "        next_obs, next_done, storage, action_key, global_step = rollout(agent_state, next_obs, next_done, storage, action_key, global_step, writer)\n",
    "        storage = compute_gae(agent_state, next_obs, next_done, storage)\n",
    "        agent_state, loss, pg_loss, v_loss, entropy_loss, approx_kl, explained_var, permutation_key = update_ppo(agent_state, storage, permutation_key)\n",
    "\n",
    "        writer.scalar(\"charts/learning_rate\", agent_state.opt_state[1].hyperparams[\"learning_rate\"].item(), global_step)\n",
    "        writer.scalar(\"losses/value_loss\", v_loss.item(), global_step)\n",
    "        writer.scalar(\"losses/policy_loss\", pg_loss.item(), global_step)\n",
    "        writer.scalar(\"losses/entropy\", entropy_loss.item(), global_step)\n",
    "        writer.scalar(\"losses/approx_kl\", approx_kl.item(), global_step)\n",
    "        writer.scalar(\"losses/explained_variance\", explained_var, global_step)\n",
    "        writer.scalar(\"charts/SPS\", int(global_step / (time.time() - start_time)), global_step)\n",
    "    print(colored('Training complete!', 'green'))\n",
    "except KeyboardInterrupt:\n",
    "    print(colored('Training interrupted!', 'red'))\n",
    "finally:\n",
    "    envs.close()\n",
    "    writer.close()\n",
    "    wandb.finish()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Validation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# Only works in SyncVectorEnv!\n",
    "# Get first environment from used environment pool, because it learned the value of running mean for reward and observation normalization\n",
    "test_env = envs.envs[1]\n",
    "# Make it render on the screen\n",
    "test_env.unwrapped.render_mode = 'human'\n",
    "test_env"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "test_env = gym.make(env_id, render_mode='rgb_array')\n",
    "test_env = gym.wrappers.AtariPreprocessing(test_env, grayscale_newaxis=True, scale_obs=True)\n",
    "test_env = gym.wrappers.RecordVideo(test_env, 'test_run/')\n",
    "test_env.metadata['render_fps'] = 24"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "from numpy import ndarray\n",
    "import tensorflow_probability.substrates.jax.distributions as tfp\n",
    "\n",
    "def get_action(agent_state: AgentState, obs: ndarray, key: random.PRNGKeyArray):\n",
    "    hidden = agent_state.network_fn(agent_state.params.network_params, obs)\n",
    "    action_logits = agent_state.actor_fn(agent_state.params.actor_params, hidden)\n",
    "    probs = tfp.Categorical(action_logits)\n",
    "    key, subkey = random.split(key)\n",
    "    action = probs.sample(seed=subkey)\n",
    "    return action, key"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from termcolor import colored\n",
    "\n",
    "try:\n",
    "    observation, _ = test_env.reset()\n",
    "    while True:\n",
    "        observation = np.expand_dims(observation, 0)\n",
    "        action, action_key = get_action(agent_state, observation, action_key)\n",
    "        action = action.item()\n",
    "        observation, reward, terminated, truncated, info = test_env.step(action)\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "except KeyboardInterrupt:\n",
    "    print(colored('Validation stopped!', 'red'))\n",
    "finally:\n",
    "    test_env.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Saving model for future usage"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "from flax.training import checkpoints\n",
    "\n",
    "checkpoints.save_checkpoint(ckpt_dir='checkpoints/',  # Folder to save checkpoint in\n",
    "                            target=agent_state,  # What to save. To only save parameters, use model_state.params\n",
    "                            step=update,  # Training step or other metric to save best model on\n",
    "                            prefix=f'{env_id}-',  # Checkpoint file name prefix\n",
    "                            overwrite=True   # Overwrite existing checkpoint files\n",
    "                            )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "from flax.training import checkpoints\n",
    "\n",
    "agent_state = checkpoints.restore_checkpoint(ckpt_dir='checkpoints/',  # Folder to save checkpoint in\n",
    "                               target=agent_state,  # What to save. To only save parameters, use model_state.params\n",
    "                               prefix=f'{env_id}-',  # Checkpoint file name prefix # Overwrite existing checkpoint files\n",
    "                               )"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
