{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Hyperparameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_timesteps = 10000000 # total timesteps of the experiment\n",
    "learning_rate = 2.5e-4 # the learning rate of the optimizer\n",
    "num_envs = 8 # the number of parallel environments\n",
    "num_steps = 128 # the number of steps to run in each environment per policy rollout\n",
    "gamma = 0.99 # the discount factor gamma\n",
    "gae_lambda = 0.95 # the lambda for the general advantage estimation\n",
    "num_minibatches = 4 # the number of mini batches\n",
    "update_epochs = 4 # the K epochs to update the policy\n",
    "clip_coef = 0.1 # the surrogate clipping coefficient\n",
    "ent_coef = 0.01 # coefficient of the entropy\n",
    "vf_coef = 0.5 # coefficient of the value function\n",
    "max_grad_norm = 0.5 # the maximum norm for the gradient clipping\n",
    "seed = 1 # seed for reproducible benchmarks\n",
    "exp_name = 'PPO' # unique experiment name\n",
    "env_id= \"KungFuMasterNoFrameskip-v4\" # id of the environment\n",
    "capture_video = True # whether to save video of agent gameplay\n",
    "\n",
    "batch_size = num_envs * num_steps # size of the batch after one rollout\n",
    "minibatch_size = batch_size // num_minibatches # size of the mini batch\n",
    "num_updates = total_timesteps // batch_size # the number of learning cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Disabling gpu on tensorflow (it is used internally by some flax modules)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-09 21:40:00.957287: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-09 21:40:01.741107: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-04-09 21:40:03.142506: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-04-09 21:40:03.162218: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-04-09 21:40:03.162423: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.config.experimental.set_visible_devices([], 'GPU')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Make environment"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import string\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "def make_env(env_id: string, idx: int, capture_video: bool, run_name: string):\n",
    "    def thunk():\n",
    "        if capture_video:\n",
    "            env = gym.make(env_id, render_mode='rgb_array')\n",
    "            if idx == 0:\n",
    "                env = gym.wrappers.RecordVideo(env, f\"videos/{env_id}\")\n",
    "        else:\n",
    "            env = gym.make(env_id)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        env = gym.wrappers.AtariPreprocessing(env, grayscale_newaxis=True, scale_obs=True)\n",
    "        return env\n",
    "\n",
    "    return thunk"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n",
      "/home/adrian/.venv/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:364: UserWarning: \u001B[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001B[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "envs = gym.vector.SyncVectorEnv(\n",
    "    [make_env(env_id, i, capture_video, exp_name) for i in range(num_envs)]\n",
    ") # AsyncVectorEnv is faster, but we cannot extract single environment from it\n",
    "assert isinstance(envs.single_action_space, gym.spaces.Discrete), \"only discrete action space is supported\"\n",
    "obs, _ = envs.reset()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create Agent model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import flax.linen as nn\n",
    "\n",
    "# Helper function to quickly declare linear layer with weight and bias initializers\n",
    "def linear_layer_init(features, std=np.sqrt(2), bias_const=0.0):\n",
    "    layer = nn.Dense(features=features, kernel_init=nn.initializers.orthogonal(std), bias_init=nn.initializers.constant(bias_const))\n",
    "    return layer\n",
    "\n",
    "# Helper function to quickly declare convolution layer with weight and bias initializers\n",
    "def convolution_layer_init(features, kernel_size, strides, std=np.sqrt(2), bias_const=0.0):\n",
    "    layer = nn.Conv(features=features, kernel_size=(kernel_size, kernel_size), strides=(strides, strides), padding='VALID', kernel_init=nn.initializers.orthogonal(std), bias_init=nn.initializers.constant(bias_const))\n",
    "    return layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from jax import Array\n",
    "import jax.numpy as jnp\n",
    "\n",
    "class Network(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x: Array):\n",
    "        x = nn.Sequential([\n",
    "            convolution_layer_init(32, 8, 4),\n",
    "            nn.relu,\n",
    "            convolution_layer_init(64, 4, 2),\n",
    "            nn.relu,\n",
    "            convolution_layer_init(64, 3, 1),\n",
    "            nn.relu,\n",
    "        ])(x)\n",
    "        x = jnp.reshape(x, (x.shape[0], -1))\n",
    "        return nn.Sequential([\n",
    "            linear_layer_init(512),\n",
    "            nn.relu\n",
    "        ])(x)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    action_n: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x: Array):\n",
    "        return linear_layer_init(self.action_n, std=0.01)(x)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x: Array):\n",
    "        return linear_layer_init(1, std=1)(x)\n",
    "\n",
    "network = Network()\n",
    "actor = Actor(action_n=envs.single_action_space.n) # For jit we need to declare prod outside of class\n",
    "critic = Critic()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create AgentState"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import jax.random as random\n",
    "\n",
    "# Setting seed of the environment for reproduction\n",
    "key = random.PRNGKey(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "key, network_key, actor_key, critic_key, action_key, permutation_key = random.split(key, num=6)\n",
    "\n",
    "# Initializing agent parameters\n",
    "network_params = network.init(network_key, obs)\n",
    "\n",
    "logits = network.apply(network_params, obs)\n",
    "\n",
    "actor_params = actor.init(actor_key, logits)\n",
    "critic_params = critic.init(critic_key, logits)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import optax\n",
    "\n",
    "# Anneal learning rate over time\n",
    "def linear_schedule(count):\n",
    "    frac = 1.0 - (count // (num_minibatches * update_epochs)) / num_updates\n",
    "    return learning_rate * frac\n",
    "\n",
    "tx = optax.chain(\n",
    "    optax.clip_by_global_norm(max_grad_norm),\n",
    "    optax.inject_hyperparams(optax.adamw)(\n",
    "        learning_rate=linear_schedule,\n",
    "        eps=1e-5\n",
    "    )\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from flax.core import FrozenDict\n",
    "from flax.struct import dataclass\n",
    "\n",
    "@dataclass\n",
    "class AgentParams:\n",
    "    actor_params: FrozenDict\n",
    "    critic_params: FrozenDict\n",
    "    network_params: FrozenDict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from jax import jit\n",
    "from typing import Callable\n",
    "from flax.training.train_state import TrainState\n",
    "from flax import struct\n",
    "\n",
    "# Probably jitting isn't needed as this functions should be jitted already\n",
    "actor.apply = jit(actor.apply)\n",
    "critic.apply = jit(critic.apply)\n",
    "network.apply = jit(network.apply)\n",
    "\n",
    "class AgentState(TrainState):\n",
    "    # Setting default values for agent functions to make TrainState work in jitted function\n",
    "    actor_fn: Callable = struct.field(pytree_node=False)\n",
    "    critic_fn: Callable = struct.field(pytree_node=False)\n",
    "    network_fn: Callable = struct.field(pytree_node=False)\n",
    "\n",
    "agent_state = AgentState.create(\n",
    "    params=AgentParams(\n",
    "        network_params=network_params,\n",
    "        actor_params=actor_params,\n",
    "        critic_params=critic_params\n",
    "    ),\n",
    "    tx=tx,\n",
    "    # As we have separated actor and critic we don't use apply_fn\n",
    "    apply_fn=None,\n",
    "    actor_fn=actor.apply,\n",
    "    critic_fn=critic.apply,\n",
    "    network_fn=network.apply\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Only run this if you want to continue training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "tx = optax.chain(\n",
    "    optax.clip_by_global_norm(max_grad_norm),\n",
    "    optax.inject_hyperparams(optax.adamw)(\n",
    "        learning_rate=linear_schedule,\n",
    "        eps=1e-5\n",
    "    )\n",
    ")\n",
    "\n",
    "agent_state = AgentState.create(\n",
    "    params=AgentParams(\n",
    "        actor_params=agent_state.params.actor_params,\n",
    "        critic_params=agent_state.params.critic_params,\n",
    "        network_params=agent_state.params.network_params\n",
    "    ),\n",
    "    tx=tx,\n",
    "    apply_fn=None,\n",
    "    actor_fn=actor.apply,\n",
    "    critic_fn=critic.apply,\n",
    "    network_fn=network.apply\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create storage"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Storage:\n",
    "    obs: jnp.array\n",
    "    actions: jnp.array\n",
    "    logprobs: jnp.array\n",
    "    dones: jnp.array\n",
    "    values: jnp.array\n",
    "    advantages: jnp.array\n",
    "    returns: jnp.array\n",
    "    rewards: jnp.array"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sample action"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from numpy import ndarray\n",
    "import tensorflow_probability.substrates.jax.distributions as tfp\n",
    "\n",
    "@jit\n",
    "def get_action_and_value(agent_state: AgentState, next_obs: ndarray, next_done: ndarray, storage: Storage, step: int, key: random.PRNGKeyArray):\n",
    "    hidden = agent_state.network_fn(agent_state.params.network_params, next_obs)\n",
    "    action_logits = agent_state.actor_fn(agent_state.params.actor_params, hidden)\n",
    "    value = agent_state.critic_fn(agent_state.params.critic_params, hidden)\n",
    "\n",
    "    # Sample discrete actions from Normal distribution\n",
    "    probs = tfp.Categorical(action_logits)\n",
    "    key, subkey = random.split(key)\n",
    "    action = probs.sample(seed=subkey)\n",
    "    logprob = probs.log_prob(action)\n",
    "    storage = storage.replace(\n",
    "        obs=storage.obs.at[step].set(next_obs),\n",
    "        dones=storage.dones.at[step].set(next_done),\n",
    "        actions=storage.actions.at[step].set(action),\n",
    "        logprobs=storage.logprobs.at[step].set(logprob),\n",
    "        values=storage.values.at[step].set(value.squeeze()),\n",
    "    )\n",
    "    return storage, action, key"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "@jit\n",
    "def get_action_and_value2(agent_state: AgentState, params: AgentParams, obs: ndarray, action: ndarray):\n",
    "    hidden = agent_state.network_fn(params.network_params, obs)\n",
    "    action_logits = agent_state.actor_fn(params.actor_params, hidden)\n",
    "    value = agent_state.critic_fn(params.critic_params, hidden)\n",
    "\n",
    "    probs = tfp.Categorical(action_logits)\n",
    "    return probs.log_prob(action), probs.entropy(), value.squeeze()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Rollout"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "from flax.metrics.tensorboard import SummaryWriter\n",
    "from jax import device_get\n",
    "\n",
    "def rollout(\n",
    "        agent_state: AgentState,\n",
    "        next_obs: ndarray,\n",
    "        next_done: ndarray,\n",
    "        storage: Storage,\n",
    "        key: random.PRNGKeyArray,\n",
    "        global_step: int,\n",
    "        writer: SummaryWriter,\n",
    "):\n",
    "    for step in range(0, num_steps):\n",
    "        global_step += 1 * num_envs\n",
    "        storage, action, key = get_action_and_value(agent_state, next_obs, next_done, storage, step, key)\n",
    "        next_obs, reward, terminated, truncated, infos = envs.step(device_get(action))\n",
    "        next_done = terminated | truncated\n",
    "        storage = storage.replace(rewards=storage.rewards.at[step].set(reward))\n",
    "\n",
    "        # Only print when at least 1 env is done\n",
    "        if \"final_info\" not in infos:\n",
    "            continue\n",
    "\n",
    "        for info in infos[\"final_info\"]:\n",
    "            # Skip the envs that are not done\n",
    "            if info is None:\n",
    "                continue\n",
    "            writer.scalar(\"charts/episodic_return\", info[\"episode\"][\"r\"], global_step)\n",
    "            writer.scalar(\"charts/episodic_length\", info[\"episode\"][\"l\"], global_step)\n",
    "    return next_obs, next_done, storage, key, global_step"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Compute gae"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "@jit\n",
    "def compute_gae(\n",
    "        agent_state: AgentState,\n",
    "        next_obs: ndarray,\n",
    "        next_done: ndarray,\n",
    "        storage: Storage\n",
    "):\n",
    "    # Reset advantages values\n",
    "    storage = storage.replace(advantages=storage.advantages.at[:].set(0.0))\n",
    "    hidden = agent_state.network_fn(agent_state.params.network_params, next_obs)\n",
    "    next_value = agent_state.critic_fn(agent_state.params.critic_params, hidden).squeeze()\n",
    "    # Compute advantage using generalized advantage estimate\n",
    "    lastgaelam = 0\n",
    "    for t in reversed(range(num_steps)):\n",
    "        if t == num_steps - 1:\n",
    "            nextnonterminal = 1.0 - next_done\n",
    "            nextvalues = next_value\n",
    "        else:\n",
    "            nextnonterminal = 1.0 - storage.dones[t + 1]\n",
    "            nextvalues = storage.values[t + 1]\n",
    "        delta = storage.rewards[t] + gamma * nextvalues * nextnonterminal - storage.values[t]\n",
    "        lastgaelam = delta + gamma * gae_lambda * nextnonterminal * lastgaelam\n",
    "        storage = storage.replace(advantages=storage.advantages.at[t].set(lastgaelam))\n",
    "    # Save returns as advantages + values\n",
    "    storage = storage.replace(returns=storage.advantages + storage.values)\n",
    "    return storage"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PPO loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "from jax.lax import stop_gradient\n",
    "\n",
    "@jit\n",
    "def ppo_loss(\n",
    "        agent_state: AgentState,\n",
    "        params: AgentParams,\n",
    "        obs: ndarray,\n",
    "        act: ndarray,\n",
    "        logp: ndarray,\n",
    "        adv: ndarray,\n",
    "        ret: ndarray,\n",
    "        val: ndarray,\n",
    "):\n",
    "    newlogprob, entropy, newvalue = get_action_and_value2(agent_state, params, obs, act)\n",
    "    logratio = newlogprob - logp\n",
    "    ratio = jnp.exp(logratio)\n",
    "\n",
    "    # Calculate how much policy is changing\n",
    "    approx_kl = ((ratio - 1) - logratio).mean()\n",
    "\n",
    "    # Advantage normalization\n",
    "    adv = (adv - adv.mean()) / (adv.std() + 1e-8)\n",
    "\n",
    "    # Policy loss\n",
    "    pg_loss1 = -adv * ratio\n",
    "    pg_loss2 = -adv * jnp.clip(ratio, 1 - clip_coef, 1 + clip_coef)\n",
    "    pg_loss = jnp.maximum(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "    # Value loss\n",
    "    v_loss_unclipped = (newvalue - ret) ** 2\n",
    "    v_clipped = val + jnp.clip(\n",
    "        newvalue - val,\n",
    "        -clip_coef,\n",
    "        clip_coef,\n",
    "    )\n",
    "    v_loss_clipped = (v_clipped - ret) ** 2\n",
    "    v_loss_max = jnp.maximum(v_loss_unclipped, v_loss_clipped)\n",
    "    v_loss = 0.5 * v_loss_max.mean()\n",
    "\n",
    "    # Entropy loss\n",
    "    entropy_loss = entropy.mean()\n",
    "\n",
    "    # main loss as sum of each part loss\n",
    "    loss = pg_loss - ent_coef * entropy_loss + v_loss * vf_coef\n",
    "    return loss, (pg_loss, v_loss, entropy_loss, stop_gradient(approx_kl))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Update PPO"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "from jax import value_and_grad\n",
    "\n",
    "\n",
    "def update_ppo(\n",
    "        agent_state: AgentState,\n",
    "        storage: Storage,\n",
    "        key: random.PRNGKeyArray\n",
    "):\n",
    "    # Flatten collected experiences\n",
    "    b_obs = storage.obs.reshape((-1,) + envs.single_observation_space.shape)\n",
    "    b_logprobs = storage.logprobs.reshape(-1)\n",
    "    b_actions = storage.actions.reshape((-1,) + envs.single_action_space.shape)\n",
    "    b_advantages = storage.advantages.reshape(-1)\n",
    "    b_returns = storage.returns.reshape(-1)\n",
    "    b_values = storage.values.reshape(-1)\n",
    "\n",
    "    # Create function that will return gradient of the specified function\n",
    "    ppo_loss_grad_fn = jit(value_and_grad(ppo_loss, argnums=1, has_aux=True))\n",
    "\n",
    "    for epoch in range(update_epochs):\n",
    "        key, subkey = random.split(key)\n",
    "        b_inds = random.permutation(subkey, batch_size, independent=True)\n",
    "        for start in range(0, batch_size, minibatch_size):\n",
    "            end = start + minibatch_size\n",
    "            mb_inds = b_inds[start:end]\n",
    "            (loss, (pg_loss, v_loss, entropy_loss, approx_kl)), grads = ppo_loss_grad_fn(\n",
    "                agent_state,\n",
    "                agent_state.params,\n",
    "                b_obs[mb_inds],\n",
    "                b_actions[mb_inds],\n",
    "                b_logprobs[mb_inds],\n",
    "                b_advantages[mb_inds],\n",
    "                b_returns[mb_inds],\n",
    "                b_values[mb_inds],\n",
    "            )\n",
    "            # Update an agent\n",
    "            agent_state = agent_state.apply_gradients(grads=grads)\n",
    "\n",
    "    # Calculate how good an approximation of the return is the value function\n",
    "    y_pred, y_true = b_values, b_returns\n",
    "    var_y = jnp.var(y_true)\n",
    "    explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "    return agent_state, loss, pg_loss, v_loss, entropy_loss, approx_kl, explained_var, key"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train agent"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33majaskowiec\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.14.2 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.14.1"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/home/adrian/Projekty/PPO/wandb/run-20230409_214013-ppnz29cu</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/ajaskowiec/KungFuMasterNoFrameskip-v4/runs/ppnz29cu' target=\"_blank\">PPO_1_Sun_Apr_9_21:40:12_2023</a></strong> to <a href='https://wandb.ai/ajaskowiec/KungFuMasterNoFrameskip-v4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/ajaskowiec/KungFuMasterNoFrameskip-v4' target=\"_blank\">https://wandb.ai/ajaskowiec/KungFuMasterNoFrameskip-v4</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/ajaskowiec/KungFuMasterNoFrameskip-v4/runs/ppnz29cu' target=\"_blank\">https://wandb.ai/ajaskowiec/KungFuMasterNoFrameskip-v4/runs/ppnz29cu</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-09 21:40:14.732949: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/9765 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "020f3fd90dbd4438a8c1d079d61c1f64"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /home/adrian/Projekty/PPO/videos/KungFuMasterNoFrameskip-v4/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /home/adrian/Projekty/PPO/videos/KungFuMasterNoFrameskip-v4/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "t:   0%|          | 0/3365 [00:00<?, ?it/s, now=None]\u001B[A\n",
      "t:  10%|▉         | 324/3365 [00:00<00:00, 3236.32it/s, now=None]\u001B[A\n",
      "t:  19%|█▉        | 648/3365 [00:00<00:00, 3159.55it/s, now=None]\u001B[A\n",
      "t:  29%|██▊       | 965/3365 [00:00<00:00, 3146.93it/s, now=None]\u001B[A\n",
      "t:  38%|███▊      | 1280/3365 [00:00<00:00, 3079.61it/s, now=None]\u001B[A\n",
      "t:  49%|████▊     | 1636/3365 [00:00<00:00, 3247.66it/s, now=None]\u001B[A\n",
      "t:  58%|█████▊    | 1962/3365 [00:00<00:00, 3183.28it/s, now=None]\u001B[A\n",
      "t:  69%|██████▉   | 2314/3365 [00:00<00:00, 3287.70it/s, now=None]\u001B[A\n",
      "t:  79%|███████▊  | 2644/3365 [00:00<00:00, 3256.37it/s, now=None]\u001B[A\n",
      "t:  89%|████████▊ | 2984/3365 [00:00<00:00, 3297.48it/s, now=None]\u001B[A\n",
      "t:  99%|█████████▊| 3315/3365 [00:01<00:00, 3164.57it/s, now=None]\u001B[A\n",
      "                                                                  \u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/adrian/Projekty/PPO/videos/KungFuMasterNoFrameskip-v4/rl-video-episode-0.mp4\n",
      "Moviepy - Building video /home/adrian/Projekty/PPO/videos/KungFuMasterNoFrameskip-v4/rl-video-episode-1.mp4.\n",
      "Moviepy - Writing video /home/adrian/Projekty/PPO/videos/KungFuMasterNoFrameskip-v4/rl-video-episode-1.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "t:   0%|          | 0/3374 [00:00<?, ?it/s, now=None]\u001B[A\n",
      "t:  11%|█         | 376/3374 [00:00<00:00, 3753.24it/s, now=None]\u001B[A\n",
      "t:  22%|██▏       | 752/3374 [00:00<00:00, 3357.63it/s, now=None]\u001B[A\n",
      "t:  32%|███▏      | 1091/3374 [00:00<00:00, 3346.14it/s, now=None]\u001B[A\n",
      "t:  42%|████▏     | 1428/3374 [00:00<00:00, 3204.70it/s, now=None]\u001B[A\n",
      "t:  52%|█████▏    | 1754/3374 [00:00<00:00, 3221.53it/s, now=None]\u001B[A\n",
      "t:  62%|██████▏   | 2104/3374 [00:00<00:00, 3309.42it/s, now=None]\u001B[A\n",
      "t:  72%|███████▏  | 2443/3374 [00:00<00:00, 3334.98it/s, now=None]\u001B[A\n",
      "t:  82%|████████▏ | 2778/3374 [00:00<00:00, 3191.82it/s, now=None]\u001B[A\n",
      "t:  92%|█████████▏| 3115/3374 [00:00<00:00, 3243.29it/s, now=None]\u001B[A\n",
      "                                                                  \u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/adrian/Projekty/PPO/videos/KungFuMasterNoFrameskip-v4/rl-video-episode-1.mp4\n",
      "Moviepy - Building video /home/adrian/Projekty/PPO/videos/KungFuMasterNoFrameskip-v4/rl-video-episode-8.mp4.\n",
      "Moviepy - Writing video /home/adrian/Projekty/PPO/videos/KungFuMasterNoFrameskip-v4/rl-video-episode-8.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "t:   0%|          | 0/5727 [00:00<?, ?it/s, now=None]\u001B[A\n",
      "t:   4%|▍         | 241/5727 [00:00<00:02, 2407.65it/s, now=None]\u001B[A\n",
      "t:   8%|▊         | 482/5727 [00:00<00:02, 2316.11it/s, now=None]\u001B[A\n",
      "t:  13%|█▎        | 724/5727 [00:00<00:02, 2361.73it/s, now=None]\u001B[A\n",
      "t:  17%|█▋        | 961/5727 [00:00<00:02, 2344.95it/s, now=None]\u001B[A\n",
      "t:  21%|██        | 1196/5727 [00:00<00:01, 2285.37it/s, now=None]\u001B[A\n",
      "t:  25%|██▍       | 1429/5727 [00:00<00:01, 2298.26it/s, now=None]\u001B[A\n",
      "t:  29%|██▉       | 1660/5727 [00:00<00:01, 2260.64it/s, now=None]\u001B[A\n",
      "t:  33%|███▎      | 1905/5727 [00:00<00:01, 2318.11it/s, now=None]\u001B[A\n",
      "t:  37%|███▋      | 2138/5727 [00:00<00:01, 2249.28it/s, now=None]\u001B[A\n",
      "t:  41%|████▏     | 2367/5727 [00:01<00:01, 2258.54it/s, now=None]\u001B[A\n",
      "t:  45%|████▌     | 2594/5727 [00:01<00:01, 2237.11it/s, now=None]\u001B[A\n",
      "t:  49%|████▉     | 2818/5727 [00:01<00:01, 2168.81it/s, now=None]\u001B[A\n",
      "t:  53%|█████▎    | 3036/5727 [00:01<00:01, 2126.68it/s, now=None]\u001B[A\n",
      "t:  57%|█████▋    | 3250/5727 [00:01<00:01, 2083.26it/s, now=None]\u001B[A\n",
      "t:  61%|██████    | 3478/5727 [00:01<00:01, 2137.71it/s, now=None]\u001B[A\n",
      "t:  65%|██████▍   | 3703/5727 [00:01<00:00, 2170.20it/s, now=None]\u001B[A\n",
      "t:  68%|██████▊   | 3921/5727 [00:01<00:00, 2169.37it/s, now=None]\u001B[A\n",
      "t:  72%|███████▏  | 4143/5727 [00:01<00:00, 2183.90it/s, now=None]\u001B[A\n",
      "t:  76%|███████▌  | 4362/5727 [00:01<00:00, 2163.59it/s, now=None]\u001B[A\n",
      "t:  80%|███████▉  | 4579/5727 [00:02<00:00, 2141.66it/s, now=None]\u001B[A\n",
      "t:  84%|████████▍ | 4801/5727 [00:02<00:00, 2162.76it/s, now=None]\u001B[A\n",
      "t:  88%|████████▊ | 5023/5727 [00:02<00:00, 2177.48it/s, now=None]\u001B[A\n",
      "t:  92%|█████████▏| 5241/5727 [00:02<00:00, 2100.82it/s, now=None]\u001B[A\n",
      "t:  95%|█████████▌| 5452/5727 [00:02<00:00, 2079.27it/s, now=None]\u001B[A\n",
      "t:  99%|█████████▉| 5665/5727 [00:02<00:00, 2092.35it/s, now=None]\u001B[A\n",
      "                                                                  \u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/adrian/Projekty/PPO/videos/KungFuMasterNoFrameskip-v4/rl-video-episode-8.mp4\n",
      "Moviepy - Building video /home/adrian/Projekty/PPO/videos/KungFuMasterNoFrameskip-v4/rl-video-episode-27.mp4.\n",
      "Moviepy - Writing video /home/adrian/Projekty/PPO/videos/KungFuMasterNoFrameskip-v4/rl-video-episode-27.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "t:   0%|          | 0/4775 [00:00<?, ?it/s, now=None]\u001B[A\n",
      "t:   6%|▌         | 286/4775 [00:00<00:01, 2859.51it/s, now=None]\u001B[A\n",
      "t:  12%|█▏        | 582/4775 [00:00<00:01, 2912.69it/s, now=None]\u001B[A\n",
      "t:  18%|█▊        | 874/4775 [00:00<00:01, 2868.60it/s, now=None]\u001B[A\n",
      "t:  24%|██▍       | 1161/4775 [00:00<00:01, 2824.55it/s, now=None]\u001B[A\n",
      "t:  30%|███       | 1444/4775 [00:00<00:01, 2781.90it/s, now=None]\u001B[A\n",
      "t:  36%|███▌      | 1723/4775 [00:00<00:01, 2742.09it/s, now=None]\u001B[A\n",
      "t:  42%|████▏     | 2000/4775 [00:00<00:01, 2750.51it/s, now=None]\u001B[A\n",
      "t:  48%|████▊     | 2276/4775 [00:00<00:00, 2663.59it/s, now=None]\u001B[A\n",
      "t:  53%|█████▎    | 2543/4775 [00:00<00:00, 2663.48it/s, now=None]\u001B[A\n",
      "t:  59%|█████▉    | 2812/4775 [00:01<00:00, 2670.53it/s, now=None]\u001B[A\n",
      "t:  65%|██████▍   | 3080/4775 [00:01<00:00, 2653.17it/s, now=None]\u001B[A\n",
      "t:  70%|███████   | 3346/4775 [00:01<00:00, 2543.52it/s, now=None]\u001B[A\n",
      "t:  75%|███████▌  | 3602/4775 [00:01<00:00, 2533.69it/s, now=None]\u001B[A\n",
      "t:  81%|████████  | 3856/4775 [00:01<00:00, 2509.28it/s, now=None]\u001B[A\n",
      "t:  86%|████████▌ | 4108/4775 [00:01<00:00, 2494.05it/s, now=None]\u001B[A\n",
      "t:  91%|█████████▏| 4358/4775 [00:01<00:00, 2384.21it/s, now=None]\u001B[A\n",
      "t:  96%|█████████▋| 4598/4775 [00:01<00:00, 2317.30it/s, now=None]\u001B[A\n",
      "                                                                  \u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/adrian/Projekty/PPO/videos/KungFuMasterNoFrameskip-v4/rl-video-episode-27.mp4\n",
      "Moviepy - Building video /home/adrian/Projekty/PPO/videos/KungFuMasterNoFrameskip-v4/rl-video-episode-64.mp4.\n",
      "Moviepy - Writing video /home/adrian/Projekty/PPO/videos/KungFuMasterNoFrameskip-v4/rl-video-episode-64.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "t:   0%|          | 0/5078 [00:00<?, ?it/s, now=None]\u001B[A\n",
      "t:   6%|▌         | 287/5078 [00:00<00:01, 2866.41it/s, now=None]\u001B[A\n",
      "t:  11%|█▏        | 574/5078 [00:00<00:01, 2722.58it/s, now=None]\u001B[A\n",
      "t:  17%|█▋        | 847/5078 [00:00<00:01, 2672.70it/s, now=None]\u001B[A\n",
      "t:  22%|██▏       | 1116/5078 [00:00<00:01, 2675.48it/s, now=None]\u001B[A\n",
      "t:  27%|██▋       | 1384/5078 [00:00<00:01, 2658.75it/s, now=None]\u001B[A\n",
      "t:  32%|███▏      | 1650/5078 [00:00<00:01, 2658.65it/s, now=None]\u001B[A\n",
      "t:  38%|███▊      | 1916/5078 [00:00<00:01, 2619.58it/s, now=None]\u001B[A\n",
      "t:  43%|████▎     | 2179/5078 [00:00<00:01, 2581.04it/s, now=None]\u001B[A\n",
      "t:  48%|████▊     | 2443/5078 [00:00<00:01, 2597.55it/s, now=None]\u001B[A\n",
      "t:  53%|█████▎    | 2706/5078 [00:01<00:00, 2606.03it/s, now=None]\u001B[A\n",
      "t:  58%|█████▊    | 2967/5078 [00:01<00:00, 2594.90it/s, now=None]\u001B[A\n",
      "t:  64%|██████▎   | 3227/5078 [00:01<00:00, 2587.79it/s, now=None]\u001B[A\n",
      "t:  69%|██████▊   | 3486/5078 [00:01<00:00, 2571.09it/s, now=None]\u001B[A\n",
      "t:  74%|███████▎  | 3744/5078 [00:01<00:00, 2546.02it/s, now=None]\u001B[A\n",
      "t:  79%|███████▉  | 3999/5078 [00:01<00:00, 2516.45it/s, now=None]\u001B[A\n",
      "t:  84%|████████▎ | 4251/5078 [00:01<00:00, 2411.49it/s, now=None]\u001B[A\n",
      "t:  88%|████████▊ | 4493/5078 [00:01<00:00, 2410.66it/s, now=None]\u001B[A\n",
      "t:  93%|█████████▎| 4735/5078 [00:01<00:00, 2377.45it/s, now=None]\u001B[A\n",
      "t:  98%|█████████▊| 4974/5078 [00:01<00:00, 2355.47it/s, now=None]\u001B[A\n",
      "                                                                  \u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/adrian/Projekty/PPO/videos/KungFuMasterNoFrameskip-v4/rl-video-episode-64.mp4\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import wandb\n",
    "from tqdm.notebook import tqdm\n",
    "from termcolor import colored\n",
    "import signal\n",
    "\n",
    "# Make kernel interrupt be handled as normal python error\n",
    "signal.signal(signal.SIGINT, signal.default_int_handler)\n",
    "\n",
    "run_name = f\"{exp_name}_{seed}_{time.asctime(time.localtime(time.time())).replace('  ', ' ').replace(' ', '_')}\"\n",
    "wandb.init(\n",
    "    project=env_id,\n",
    "    sync_tensorboard=True,\n",
    "    name=run_name,\n",
    "    save_code=True,\n",
    "    monitor_gym=capture_video,\n",
    "    config={\n",
    "        'total_timesteps': total_timesteps,\n",
    "        'learning_rate': learning_rate,\n",
    "        'num_envs': num_envs,\n",
    "        'num_steps': num_steps,\n",
    "        'gamma': gamma,\n",
    "        'gae_lambda': gae_lambda,\n",
    "        'num_minibatches': num_minibatches,\n",
    "        'update_epochs': update_epochs,\n",
    "        'clip_coef': clip_coef,\n",
    "        'ent_coef': ent_coef,\n",
    "        'vf_coef': vf_coef,\n",
    "        'max_grad_norm': max_grad_norm,\n",
    "        'seed': seed,\n",
    "        'batch_size': batch_size,\n",
    "        'minibatch_size': minibatch_size,\n",
    "        'num_updates': num_updates,\n",
    "    }\n",
    ")\n",
    "writer = SummaryWriter(f'runs/{env_id}/{run_name}')\n",
    "\n",
    "# Initialize the storage\n",
    "storage = Storage(\n",
    "    obs=jnp.zeros((num_steps, num_envs) + envs.single_observation_space.shape),\n",
    "    actions=jnp.zeros((num_steps, num_envs) + envs.single_action_space.shape),\n",
    "    logprobs=jnp.zeros((num_steps, num_envs)),\n",
    "    dones=jnp.zeros((num_steps, num_envs)),\n",
    "    values=jnp.zeros((num_steps, num_envs)),\n",
    "    advantages=jnp.zeros((num_steps, num_envs)),\n",
    "    returns=jnp.zeros((num_steps, num_envs)),\n",
    "    rewards=jnp.zeros((num_steps, num_envs)),\n",
    ")\n",
    "global_step = 0\n",
    "start_time = time.time()\n",
    "next_obs, _ = envs.reset(seed=seed)\n",
    "next_done = jnp.zeros(num_envs)\n",
    "\n",
    "try:\n",
    "    for update in tqdm(range(1, num_updates + 1)):\n",
    "        next_obs, next_done, storage, action_key, global_step = rollout(agent_state, next_obs, next_done, storage, action_key, global_step, writer)\n",
    "        storage = compute_gae(agent_state, next_obs, next_done, storage)\n",
    "        agent_state, loss, pg_loss, v_loss, entropy_loss, approx_kl, explained_var, permutation_key = update_ppo(agent_state, storage, permutation_key)\n",
    "\n",
    "        writer.scalar(\"charts/learning_rate\", agent_state.opt_state[1].hyperparams[\"learning_rate\"].item(), global_step)\n",
    "        writer.scalar(\"losses/value_loss\", v_loss.item(), global_step)\n",
    "        writer.scalar(\"losses/policy_loss\", pg_loss.item(), global_step)\n",
    "        writer.scalar(\"losses/entropy\", entropy_loss.item(), global_step)\n",
    "        writer.scalar(\"losses/approx_kl\", approx_kl.item(), global_step)\n",
    "        writer.scalar(\"losses/explained_variance\", explained_var, global_step)\n",
    "        writer.scalar(\"charts/SPS\", int(global_step / (time.time() - start_time)), global_step)\n",
    "    print(colored('Training complete!', 'green'))\n",
    "except KeyboardInterrupt:\n",
    "    print(colored('Training interrupted!', 'red'))\n",
    "finally:\n",
    "    envs.close()\n",
    "    writer.close()\n",
    "    wandb.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Validation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Only works in SyncVectorEnv!\n",
    "# Get first environment from used environment pool, because it learned the value of running mean for reward and observation normalization\n",
    "test_env = envs.envs[1]\n",
    "# Make it render on the screen\n",
    "test_env.unwrapped.render_mode = 'human'\n",
    "test_env"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_action(agent_state: AgentState, obs: ndarray, key: random.PRNGKeyArray):\n",
    "    hidden = agent_state.network_fn(agent_state.params.network_params, obs)\n",
    "    action_logits = agent_state.actor_fn(agent_state.params.actor_params, hidden)\n",
    "    probs = tfp.Categorical(action_logits)\n",
    "    key, subkey = random.split(key)\n",
    "    action = probs.sample(seed=subkey)\n",
    "    return action, key"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "try:\n",
    "    for i in range(10):\n",
    "        observation, _ = test_env.reset()\n",
    "        while True:\n",
    "            observation = observation.reshape((1, -1))\n",
    "            action, action_key = get_action(agent_state, observation, action_key)\n",
    "            action = action.item()\n",
    "            observation, reward, terminated, truncated, info = test_env.step(action)\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "except KeyboardInterrupt:\n",
    "    print(colored('Validation stopped!', 'red'))\n",
    "finally:\n",
    "    test_env.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Saving model for future usage"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from flax.training import checkpoints\n",
    "\n",
    "checkpoints.save_checkpoint(ckpt_dir='checkpoints/',  # Folder to save checkpoint in\n",
    "                            target=agent_state,  # What to save. To only save parameters, use model_state.params\n",
    "                            step=update,  # Training step or other metric to save best model on\n",
    "                            prefix=env_id,  # Checkpoint file name prefix\n",
    "                            overwrite=True   # Overwrite existing checkpoint files\n",
    "                            )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
