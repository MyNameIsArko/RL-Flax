{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Hyperparameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_timesteps = 8000000 # total timesteps of the experiment\n",
    "learning_rate = 3e-4 # the learning rate of the optimizer\n",
    "num_envs = 1 # the number of parallel environments\n",
    "num_steps = 2048 # the number of steps to run in each environment per policy rollout\n",
    "gamma = 0.99 # the discount factor gamma\n",
    "gae_lambda = 0.95 # the lambda for the general advantage estimation\n",
    "num_minibatches = 32 # the number of mini batches\n",
    "update_epochs = 10 # the K epochs to update the policy\n",
    "clip_coef = 0.2 # the surrogate clipping coefficient\n",
    "ent_coef = 0.0 # coefficient of the entropy\n",
    "vf_coef = 0.5 # coefficient of the value function\n",
    "max_grad_norm = 0.5 # the maximum norm for the gradient clipping\n",
    "seed = 1 # seed for reproducible benchmarks\n",
    "exp_name = 'PPO' # unique experiment name\n",
    "env_id= \"Pusher-v4\" # id of the environment\n",
    "\n",
    "batch_size = num_envs * num_steps # size of the batch after one rollout\n",
    "minibatch_size = batch_size // num_minibatches # size of the mini batch\n",
    "num_updates = total_timesteps // batch_size # the number of learning cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Disabling gpu on tensorflow (it is used internally by some flax modules)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-09 11:30:18.592860: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-09 11:30:19.363914: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-04-09 11:30:20.283482: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-04-09 11:30:20.303006: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-04-09 11:30:20.303214: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.config.experimental.set_visible_devices([], 'GPU')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Make environment"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import string\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "def make_env(env_id: string, gamma: float):\n",
    "    def thunk():\n",
    "        env = gym.make(env_id)\n",
    "        env = gym.wrappers.FlattenObservation(env)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        env = gym.wrappers.ClipAction(env)\n",
    "        env = gym.wrappers.NormalizeObservation(env)\n",
    "        env = gym.wrappers.TransformObservation(env, lambda obs: np.clip(obs, -10, 10))\n",
    "        env = gym.wrappers.NormalizeReward(env, gamma=gamma)\n",
    "        env = gym.wrappers.TransformReward(env, lambda reward: np.clip(reward, -10, 10))\n",
    "        return env\n",
    "\n",
    "    return thunk"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "envs = gym.vector.SyncVectorEnv(\n",
    "    [make_env(env_id, gamma) for i in range(num_envs)]\n",
    ") # AsyncVectorEnv is faster, but we cannot extract single environment from it\n",
    "assert isinstance(envs.single_action_space, gym.spaces.Box), \"only continuous action space is supported\"\n",
    "obs, _ = envs.reset()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create Agent model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import flax.linen as nn\n",
    "\n",
    "# Helper function to quickly declare linear layer with weight and bias initializers\n",
    "def linear_layer_init(features, std=np.sqrt(2), bias_const=0.0):\n",
    "    layer = nn.Dense(features=features, kernel_init=nn.initializers.orthogonal(std), bias_init=nn.initializers.constant(bias_const))\n",
    "    return layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from jax import Array\n",
    "import jax.numpy as jnp\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    action_shape_prod: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x: Array):\n",
    "        action_mean = nn.Sequential([\n",
    "            linear_layer_init(64),\n",
    "            nn.tanh,\n",
    "            linear_layer_init(64),\n",
    "            nn.tanh,\n",
    "            linear_layer_init(self.action_shape_prod, std=0.01),\n",
    "        ])(x)\n",
    "        actor_logstd = self.param('logstd', nn.initializers.zeros, (1, self.action_shape_prod))\n",
    "        action_logstd = jnp.broadcast_to(actor_logstd, action_mean.shape) # Make logstd the same shape as actions\n",
    "        return action_mean, action_logstd\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x: Array):\n",
    "        return nn.Sequential([\n",
    "            linear_layer_init(64),\n",
    "            nn.tanh,\n",
    "            linear_layer_init(64),\n",
    "            nn.tanh,\n",
    "            linear_layer_init(1, std=1.0),\n",
    "        ])(x)\n",
    "\n",
    "actor = Actor(action_shape_prod=np.array(envs.single_action_space.shape).prod()) # For jit we need to declare prod outside of class\n",
    "critic = Critic()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create AgentState"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import jax.random as random\n",
    "\n",
    "# Setting seed of the environment for reproduction\n",
    "key = random.PRNGKey(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "key, actor_key, critic_key, action_key, permutation_key = random.split(key, num=5)\n",
    "\n",
    "# Initializing agent parameters\n",
    "actor_params = actor.init(actor_key, obs)\n",
    "critic_params = critic.init(critic_key, obs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import optax\n",
    "\n",
    "# Anneal learning rate over time\n",
    "def linear_schedule(count):\n",
    "    frac = 1.0 - (count // (num_minibatches * update_epochs)) / num_updates\n",
    "    return learning_rate * frac\n",
    "\n",
    "tx = optax.chain(\n",
    "    optax.clip_by_global_norm(max_grad_norm),\n",
    "    optax.inject_hyperparams(optax.adamw)(\n",
    "        learning_rate=linear_schedule,\n",
    "        eps=1e-5\n",
    "    )\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from flax.core import FrozenDict\n",
    "from flax.struct import dataclass\n",
    "\n",
    "@dataclass\n",
    "class AgentParams:\n",
    "    actor_params: FrozenDict\n",
    "    critic_params: FrozenDict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from jax import jit\n",
    "from typing import Callable\n",
    "from flax.training.train_state import TrainState\n",
    "from flax import struct\n",
    "\n",
    "# Probably jitting isn't needed as this functions should be jitted already\n",
    "actor.apply = jit(actor.apply)\n",
    "critic.apply = jit(critic.apply)\n",
    "\n",
    "class AgentState(TrainState):\n",
    "    # Setting default values for agent functions to make TrainState work in jitted function\n",
    "    actor_fn: Callable = struct.field(pytree_node=False)\n",
    "    critic_fn: Callable = struct.field(pytree_node=False)\n",
    "\n",
    "agent_state = AgentState.create(\n",
    "    params=AgentParams(\n",
    "        actor_params=actor_params,\n",
    "        critic_params=critic_params\n",
    "    ),\n",
    "    tx=tx,\n",
    "    # As we have separated actor and critic we don't use apply_fn\n",
    "    apply_fn=None,\n",
    "    actor_fn=actor.apply,\n",
    "    critic_fn=critic.apply\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Only run this if you want to continue training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "tx = optax.chain(\n",
    "    optax.clip_by_global_norm(max_grad_norm),\n",
    "    optax.inject_hyperparams(optax.adamw)(\n",
    "        learning_rate=linear_schedule,\n",
    "        eps=1e-5\n",
    "    )\n",
    ")\n",
    "\n",
    "agent_state = AgentState.create(\n",
    "    params=AgentParams(\n",
    "        actor_params=agent_state.params.actor_params,\n",
    "        critic_params=agent_state.params.critic_params\n",
    "    ),\n",
    "    tx=tx,\n",
    "    apply_fn=None,\n",
    "    actor_fn=actor.apply,\n",
    "    critic_fn=critic.apply\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create storage"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Storage:\n",
    "    obs: jnp.array\n",
    "    actions: jnp.array\n",
    "    logprobs: jnp.array\n",
    "    dones: jnp.array\n",
    "    values: jnp.array\n",
    "    advantages: jnp.array\n",
    "    returns: jnp.array\n",
    "    rewards: jnp.array"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sample action"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from numpy import ndarray\n",
    "import tensorflow_probability.substrates.jax.distributions as tfp\n",
    "\n",
    "@jit\n",
    "def get_action_and_value(agent_state: AgentState, next_obs: ndarray, next_done: ndarray, storage: Storage, step: int, key: random.PRNGKeyArray):\n",
    "    action_mean, action_logstd = agent_state.actor_fn(agent_state.params.actor_params, next_obs)\n",
    "    value = agent_state.critic_fn(agent_state.params.critic_params, next_obs)\n",
    "    action_std = jnp.exp(action_logstd)\n",
    "\n",
    "    # Sample continuous actions from Normal distribution\n",
    "    probs = tfp.Normal(action_mean, action_std)\n",
    "    key, subkey = random.split(key)\n",
    "    action = probs.sample(seed=subkey)\n",
    "    logprob = probs.log_prob(action).sum(1)\n",
    "    storage = storage.replace(\n",
    "        obs=storage.obs.at[step].set(next_obs),\n",
    "        dones=storage.dones.at[step].set(next_done),\n",
    "        actions=storage.actions.at[step].set(action),\n",
    "        logprobs=storage.logprobs.at[step].set(logprob),\n",
    "        values=storage.values.at[step].set(value.squeeze()),\n",
    "    )\n",
    "    return storage, action, key"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "@jit\n",
    "def get_action_and_value2(agent_state: AgentState, params: AgentParams, obs: ndarray, action: ndarray):\n",
    "    action_mean, action_logstd = agent_state.actor_fn(params.actor_params, obs)\n",
    "    value = agent_state.critic_fn(params.critic_params, obs)\n",
    "    action_std = jnp.exp(action_logstd)\n",
    "\n",
    "    probs = tfp.Normal(action_mean, action_std)\n",
    "    return probs.log_prob(action).sum(1), probs.entropy().sum(1), value.squeeze()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Rollout"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "from flax.metrics.tensorboard import SummaryWriter\n",
    "from jax import device_get\n",
    "\n",
    "def rollout(\n",
    "        agent_state: AgentState,\n",
    "        next_obs: ndarray,\n",
    "        next_done: ndarray,\n",
    "        storage: Storage,\n",
    "        key: random.PRNGKeyArray,\n",
    "        global_step: int,\n",
    "        writer: SummaryWriter,\n",
    "):\n",
    "    for step in range(0, num_steps):\n",
    "        global_step += 1 * num_envs\n",
    "        storage, action, key = get_action_and_value(agent_state, next_obs, next_done, storage, step, key)\n",
    "        next_obs, reward, terminated, truncated, infos = envs.step(device_get(action))\n",
    "        next_done = terminated | truncated\n",
    "        storage = storage.replace(rewards=storage.rewards.at[step].set(reward))\n",
    "\n",
    "        # Only print when at least 1 env is done\n",
    "        if \"final_info\" not in infos:\n",
    "            continue\n",
    "\n",
    "        for info in infos[\"final_info\"]:\n",
    "            # Skip the envs that are not done\n",
    "            if info is None:\n",
    "                continue\n",
    "            writer.scalar(\"charts/episodic_return\", info[\"episode\"][\"r\"], global_step)\n",
    "            writer.scalar(\"charts/episodic_length\", info[\"episode\"][\"l\"], global_step)\n",
    "    return next_obs, next_done, storage, key, global_step"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Compute gae"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "@jit\n",
    "def compute_gae(\n",
    "        agent_state: AgentState,\n",
    "        next_obs: ndarray,\n",
    "        next_done: ndarray,\n",
    "        storage: Storage\n",
    "):\n",
    "    # Reset advantages values\n",
    "    storage = storage.replace(advantages=storage.advantages.at[:].set(0.0))\n",
    "    next_value = agent_state.critic_fn(agent_state.params.critic_params, next_obs).squeeze()\n",
    "    # Compute advantage using generalized advantage estimate\n",
    "    lastgaelam = 0\n",
    "    for t in reversed(range(num_steps)):\n",
    "        if t == num_steps - 1:\n",
    "            nextnonterminal = 1.0 - next_done\n",
    "            nextvalues = next_value\n",
    "        else:\n",
    "            nextnonterminal = 1.0 - storage.dones[t + 1]\n",
    "            nextvalues = storage.values[t + 1]\n",
    "        delta = storage.rewards[t] + gamma * nextvalues * nextnonterminal - storage.values[t]\n",
    "        lastgaelam = delta + gamma * gae_lambda * nextnonterminal * lastgaelam\n",
    "        storage = storage.replace(advantages=storage.advantages.at[t].set(lastgaelam))\n",
    "    # Save returns as advantages + values\n",
    "    storage = storage.replace(returns=storage.advantages + storage.values)\n",
    "    return storage"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PPO loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "from jax.lax import stop_gradient\n",
    "\n",
    "@jit\n",
    "def ppo_loss(\n",
    "        agent_state: AgentState,\n",
    "        params: AgentParams,\n",
    "        obs: ndarray,\n",
    "        act: ndarray,\n",
    "        logp: ndarray,\n",
    "        adv: ndarray,\n",
    "        ret: ndarray,\n",
    "        val: ndarray,\n",
    "):\n",
    "    newlogprob, entropy, newvalue = get_action_and_value2(agent_state, params, obs, act)\n",
    "    logratio = newlogprob - logp\n",
    "    ratio = jnp.exp(logratio)\n",
    "\n",
    "    # Calculate how much policy is changing\n",
    "    approx_kl = ((ratio - 1) - logratio).mean()\n",
    "\n",
    "    # Advantage normalization\n",
    "    adv = (adv - adv.mean()) / (adv.std() + 1e-8)\n",
    "\n",
    "    # Policy loss\n",
    "    pg_loss1 = -adv * ratio\n",
    "    pg_loss2 = -adv * jnp.clip(ratio, 1 - clip_coef, 1 + clip_coef)\n",
    "    pg_loss = jnp.maximum(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "    # Value loss\n",
    "    v_loss_unclipped = (newvalue - ret) ** 2\n",
    "    v_clipped = val + jnp.clip(\n",
    "        newvalue - val,\n",
    "        -clip_coef,\n",
    "        clip_coef,\n",
    "    )\n",
    "    v_loss_clipped = (v_clipped - ret) ** 2\n",
    "    v_loss_max = jnp.maximum(v_loss_unclipped, v_loss_clipped)\n",
    "    v_loss = 0.5 * v_loss_max.mean()\n",
    "\n",
    "    # Entropy loss\n",
    "    entropy_loss = entropy.mean()\n",
    "\n",
    "    # main loss as sum of each part loss\n",
    "    loss = pg_loss - ent_coef * entropy_loss + v_loss * vf_coef\n",
    "    return loss, (pg_loss, v_loss, entropy_loss, stop_gradient(approx_kl))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Update PPO"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "from jax import value_and_grad\n",
    "\n",
    "\n",
    "def update_ppo(\n",
    "        agent_state: AgentState,\n",
    "        storage: Storage,\n",
    "        key: random.PRNGKeyArray\n",
    "):\n",
    "    # Flatten collected experiences\n",
    "    b_obs = storage.obs.reshape((-1,) + envs.single_observation_space.shape)\n",
    "    b_logprobs = storage.logprobs.reshape(-1)\n",
    "    b_actions = storage.actions.reshape((-1,) + envs.single_action_space.shape)\n",
    "    b_advantages = storage.advantages.reshape(-1)\n",
    "    b_returns = storage.returns.reshape(-1)\n",
    "    b_values = storage.values.reshape(-1)\n",
    "\n",
    "    # Create function that will return gradient of the specified function\n",
    "    ppo_loss_grad_fn = jit(value_and_grad(ppo_loss, argnums=1, has_aux=True))\n",
    "\n",
    "    for epoch in range(update_epochs):\n",
    "        key, subkey = random.split(key)\n",
    "        b_inds = random.permutation(subkey, batch_size, independent=True)\n",
    "        for start in range(0, batch_size, minibatch_size):\n",
    "            end = start + minibatch_size\n",
    "            mb_inds = b_inds[start:end]\n",
    "            (loss, (pg_loss, v_loss, entropy_loss, approx_kl)), grads = ppo_loss_grad_fn(\n",
    "                agent_state,\n",
    "                agent_state.params,\n",
    "                b_obs[mb_inds],\n",
    "                b_actions[mb_inds],\n",
    "                b_logprobs[mb_inds],\n",
    "                b_advantages[mb_inds],\n",
    "                b_returns[mb_inds],\n",
    "                b_values[mb_inds],\n",
    "            )\n",
    "            # Update an agent\n",
    "            agent_state = agent_state.apply_gradients(grads=grads)\n",
    "\n",
    "    # Calculate how good an approximation of the return is the value function\n",
    "    y_pred, y_true = b_values, b_returns\n",
    "    var_y = jnp.var(y_true)\n",
    "    explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "    return agent_state, loss, pg_loss, v_loss, entropy_loss, approx_kl, explained_var, key"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train agent"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import time\n",
    "import wandb\n",
    "from tqdm.notebook import tqdm\n",
    "from termcolor import colored\n",
    "import signal\n",
    "\n",
    "# Make kernel interrupt be handled as normal python error\n",
    "signal.signal(signal.SIGINT, signal.default_int_handler)\n",
    "\n",
    "run_name = f\"{exp_name}_{seed}_{time.asctime(time.localtime(time.time())).replace('  ', ' ').replace(' ', '_')}\"\n",
    "wandb.init(\n",
    "    project=env_id,\n",
    "    sync_tensorboard=True,\n",
    "    name=run_name,\n",
    "    save_code=True,\n",
    "    config={\n",
    "        'total_timesteps': total_timesteps,\n",
    "        'learning_rate': learning_rate,\n",
    "        'num_envs': num_envs,\n",
    "        'num_steps': num_steps,\n",
    "        'gamma': gamma,\n",
    "        'gae_lambda': gae_lambda,\n",
    "        'num_minibatches': num_minibatches,\n",
    "        'update_epochs': update_epochs,\n",
    "        'clip_coef': clip_coef,\n",
    "        'ent_coef': ent_coef,\n",
    "        'vf_coef': vf_coef,\n",
    "        'max_grad_norm': max_grad_norm,\n",
    "        'seed': seed,\n",
    "        'batch_size': batch_size,\n",
    "        'minibatch_size': minibatch_size,\n",
    "        'num_updates': num_updates,\n",
    "    }\n",
    ")\n",
    "writer = SummaryWriter(f'runs/{env_id}/{run_name}')\n",
    "\n",
    "# Initialize the storage\n",
    "storage = Storage(\n",
    "    obs=jnp.zeros((num_steps, num_envs) + envs.single_observation_space.shape),\n",
    "    actions=jnp.zeros((num_steps, num_envs) + envs.single_action_space.shape),\n",
    "    logprobs=jnp.zeros((num_steps, num_envs)),\n",
    "    dones=jnp.zeros((num_steps, num_envs)),\n",
    "    values=jnp.zeros((num_steps, num_envs)),\n",
    "    advantages=jnp.zeros((num_steps, num_envs)),\n",
    "    returns=jnp.zeros((num_steps, num_envs)),\n",
    "    rewards=jnp.zeros((num_steps, num_envs)),\n",
    ")\n",
    "global_step = 0\n",
    "start_time = time.time()\n",
    "next_obs, _ = envs.reset(seed=seed)\n",
    "next_done = jnp.zeros(num_envs)\n",
    "\n",
    "try:\n",
    "    for update in tqdm(range(1, num_updates + 1)):\n",
    "        next_obs, next_done, storage, action_key, global_step = rollout(agent_state, next_obs, next_done, storage, action_key, global_step, writer)\n",
    "        storage = compute_gae(agent_state, next_obs, next_done, storage)\n",
    "        agent_state, loss, pg_loss, v_loss, entropy_loss, approx_kl, explained_var, permutation_key = update_ppo(agent_state, storage, permutation_key)\n",
    "\n",
    "        writer.scalar(\"charts/learning_rate\", agent_state.opt_state[1].hyperparams[\"learning_rate\"].item(), global_step)\n",
    "        writer.scalar(\"losses/value_loss\", v_loss.item(), global_step)\n",
    "        writer.scalar(\"losses/policy_loss\", pg_loss.item(), global_step)\n",
    "        writer.scalar(\"losses/entropy\", entropy_loss.item(), global_step)\n",
    "        writer.scalar(\"losses/approx_kl\", approx_kl.item(), global_step)\n",
    "        writer.scalar(\"losses/explained_variance\", explained_var, global_step)\n",
    "        writer.scalar(\"charts/SPS\", int(global_step / (time.time() - start_time)), global_step)\n",
    "    print(colored('Training complete!', 'green'))\n",
    "except KeyboardInterrupt:\n",
    "    print(colored('Training interrupted!', 'red'))\n",
    "finally:\n",
    "    envs.close()\n",
    "    writer.close()\n",
    "    wandb.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Validation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "<TransformReward<NormalizeReward<TransformObservation<NormalizeObservation<ClipAction<RecordEpisodeStatistics<FlattenObservation<TimeLimit<OrderEnforcing<PassiveEnvChecker<PusherEnv<Pusher-v4>>>>>>>>>>>>"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only works in SyncVectorEnv!\n",
    "# Get first environment from used environment pool, because it learned the value of running mean for reward and observation normalization\n",
    "test_env = envs.envs[0]\n",
    "# Make it render on the screen\n",
    "test_env.unwrapped.render_mode = 'human'\n",
    "test_env"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "def get_action(agent_state: AgentState, obs: ndarray, key: random.PRNGKeyArray):\n",
    "    action_mean, action_logstd = agent_state.actor_fn(agent_state.params.actor_params, obs)\n",
    "    action_std = jnp.exp(action_logstd)\n",
    "    probs = tfp.Normal(action_mean, action_std)\n",
    "    key, subkey = random.split(key)\n",
    "    action = probs.sample(seed=subkey)\n",
    "    return action, key"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "try:\n",
    "    for i in range(10):\n",
    "        observation, _ = test_env.reset()\n",
    "        while True:\n",
    "            observation = observation.reshape((1, -1))\n",
    "            action, action_key = get_action(agent_state, observation, action_key)\n",
    "            action = action.reshape((-1))\n",
    "            observation, reward, terminated, truncated, info = test_env.step(action)\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "except KeyboardInterrupt:\n",
    "    print(colored('Validation stopped!', 'red'))\n",
    "finally:\n",
    "    test_env.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Saving model for future usage"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from flax.training import checkpoints\n",
    "\n",
    "checkpoints.save_checkpoint(ckpt_dir='checkpoints/',  # Folder to save checkpoint in\n",
    "                            target=agent_state,  # What to save. To only save parameters, use model_state.params\n",
    "                            step=1693,  # Training step or other metric to save best model on\n",
    "                            prefix=f'{env_id}-',  # Checkpoint file name prefix\n",
    "                            overwrite=True   # Overwrite existing checkpoint files\n",
    "                            )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
